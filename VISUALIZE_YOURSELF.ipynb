{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Fitbit_json_file_lists():\n",
    "    '''\n",
    "        takes in directory containing json files for heart rate\n",
    "        returns a list of paths to each individual file, as strings,\n",
    "        for easy consumption by next function\n",
    "    '''\n",
    "    path = str(pathlib.Path().absolute())\n",
    "    heart_jsons = []\n",
    "    calorie_jsons = []\n",
    "    location_kmls = []\n",
    "\n",
    "    for loc, dirs, files in os.walk('./json_heart_data'):\n",
    "        print('\\nvisiting location:', loc, '\\n')\n",
    "        # pass the current iteration's object pair (ignoring dirs)\n",
    "        # to our method dedicated to looking at a list of files \n",
    "        # in any arbitrary file location\n",
    "        print('FILES:', files, '\\n')\n",
    "        regexp = re.compile('(\\.json)$')\n",
    "        for f in files:\n",
    "            # rebuld the file name from the given directory name\n",
    "            # and the OS-specific file path separator we can get with os.sep\n",
    "            # and the filename we get from our iterator\n",
    "            fn = 'json_heart_data' + str(os.sep) + f\n",
    "            if os.path.isfile(fn):\n",
    "                match = re.search(regexp, fn)\n",
    "                if match:\n",
    "                    if match.group(0) == '.json':\n",
    "                        print('file name:', fn, ' size:', os.path.getsize(fn),'B ', end=' ')\n",
    "                        print('extension:', match.group(0))\n",
    "                        heart_jsons.append(path + '/' + fn)\n",
    "\n",
    "    for loc, dirs, files in os.walk('./json_calorie_data'):\n",
    "        print('\\nvisiting location:', loc, '\\n')\n",
    "        print('FILES:', files, '\\n')\n",
    "        regexp = re.compile('(\\.json)$')\n",
    "        for f in files:\n",
    "            fn = 'json_calorie_data' + str(os.sep) + f\n",
    "            if os.path.isfile(fn):\n",
    "                match = re.search(regexp, fn)\n",
    "                if match:\n",
    "                    if match.group(0) == '.json':\n",
    "                        print('file name:', fn, ' size:', os.path.getsize(fn),'B ', end=' ')\n",
    "                        print('extension:', match.group(0))\n",
    "                        calorie_jsons.append(path + '/' + fn)\n",
    "\n",
    "    for loc, dirs, files in os.walk('./kml_google_data'):\n",
    "        print('\\nvisiting location:', loc, '\\n')\n",
    "        print('FILES:', files, '\\n')\n",
    "        regexp = re.compile('(\\.kml)$')\n",
    "        for f in files:\n",
    "            fn = 'kml_google_data' + str(os.sep) + f\n",
    "            if os.path.isfile(fn):\n",
    "                match = re.search(regexp, fn)\n",
    "                if match:\n",
    "                    if match.group(0) == '.kml':\n",
    "                        print('file name:', fn, ' size:', os.path.getsize(fn),'B ', end=' ')\n",
    "                        print('extension:', match.group(0))\n",
    "                        location_kmls.append(path + '/' + fn)\n",
    "\n",
    "    return heart_jsons, calorie_jsons, location_kmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_heart_convert_csv(heart_json_path):\n",
    "    \n",
    "    heart = pd.read_json(heart_json_path)\n",
    "    # Empty lists for heartbeats/min and Fitbit 'Confidence' value\n",
    "    bpm_list = []\n",
    "    conf_list = []\n",
    "\n",
    "    # Populates empty lists with respective Fitbit values\n",
    "    for i in heart['value']:\n",
    "        bpm_list.append(i['bpm'])\n",
    "        conf_list.append(i['confidence'])\n",
    "\n",
    "    # Creates new pandas dataframes from populated lists\n",
    "    bpm_df = pd.DataFrame(bpm_list)\n",
    "    conf_df = pd.DataFrame(conf_list)\n",
    "\n",
    "    # Assigns dataframes as new series/columns in original dataframe\n",
    "    heart['BPM'] = bpm_df\n",
    "    heart['Confidence'] = conf_df\n",
    "    \n",
    "    # Exports dataframes as CSV\n",
    "    heart_csv = heart[['dateTime','BPM','Confidence']].to_csv('working_heart.csv')\n",
    "    return heart_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_calorie_convert_csv(calorie_json_path):\n",
    "\n",
    "    # Reads JSON file and renames column for Calories burned per minute\n",
    "    calor = pd.read_json(calorie_json_path)\n",
    "    calor.rename(columns={'value':'Calories'}, inplace=True)\n",
    "\n",
    "    # Exports dataframes as CSV\n",
    "    calorie_csv = calor.to_csv('working_calorie.csv')\n",
    "\n",
    "    return calorie_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_format_google_kml(location_kml_path):\n",
    "    \n",
    "    name_list = []\n",
    "    address_list = []\n",
    "    description_list = []\n",
    "    time_begin_list = []\n",
    "    time_end_list = []\n",
    "    coordinates_list = []\n",
    "    longitude_list = []\n",
    "    latitude_list = []\n",
    "    linestring_coord_list = []\n",
    "\n",
    "    with open(location_kml_path, 'r') as f: # Raw KML file is read using BeautifulSoup...\n",
    "            kml = BeautifulSoup(f, 'xml')\n",
    "            for name in kml.find_all('name')[1:]: # Relevant headers are isolated from KML...\n",
    "                name_list.append(re.sub('<.*?>', '', str(name))) # Headers are removed...\n",
    "            for address in kml.find_all('address'):\n",
    "                address_list.append(re.sub('<.*?>', '', str(address))) # Data strings appended to corresponding lists\n",
    "            for description in kml.find_all('description')[1:]: # 'Name','Description' headers for whole document excluded from parsing\n",
    "                description_list.append((re.sub('<.*?>', '', str(description)))[1:]) # Removing extra space at beginning of string\n",
    "            for time_begin in kml.find_all('begin'):\n",
    "                time_begin_list.append(re.sub('<.*?>', '', str(time_begin)))\n",
    "            for time_end in kml.find_all('end'):\n",
    "                time_end_list.append(re.sub('<.*?>', '', str(time_end)))\n",
    "            for coordinates in kml.find_all('coordinates'): # Preliminary coordinates list needed\n",
    "                coordinates_list.append(re.sub('<.*?>', '', str(coordinates)))\n",
    "\n",
    "    for coord in coordinates_list:\n",
    "        if coord.count(',') == 2: # If reading is a single coordinate (two commas detected)...\n",
    "            coord = coord[:(len(coord)) - 2] # Delete separator characters \",0\"\n",
    "            long_lat = coord.split(',') # Split by comma\n",
    "            longitude_list.append(long_lat[0]) # Add to latitude and longitude stationary lists\n",
    "            latitude_list.append(long_lat[1])\n",
    "            linestring_coord_list.append('') # Blank reading added to transitory list\n",
    "        else:\n",
    "            longitude_list.append('') # Adds blank readings to lat/long for transitory readings\n",
    "            latitude_list.append('')\n",
    "            linestring_coord_list.append(coord) # Reading added to transitory list\n",
    "\n",
    "    columns_list = ['name','address','description','TimeSpan/begin','TimeSpan/end','longitude','latitude','LineString/coordinates']\n",
    "    data_list = [name_list,address_list,description_list,time_begin_list,time_end_list,longitude_list,latitude_list,linestring_coord_list]\n",
    "    dictionary = dict(zip(columns_list, data_list))\n",
    "    formatted_location_df = pd.DataFrame(dictionary)\n",
    "    formatted_location_df.to_csv('working_location.csv')\n",
    "\n",
    "    # Exports trimmed dataframes as new CSV\n",
    "    formatted_location_csv = formatted_location_df.to_csv('working_location.csv')\n",
    "    \n",
    "    return formatted_location_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Postgres_create_DB():\n",
    "    try:\n",
    "        # Creates connection to new DB, creates if nonexistant\n",
    "        conn = psycopg2.connect(\n",
    "            database=os.getenv('POSTGRES_DB'),\n",
    "            user=os.getenv('POSTGRES_USER'),\n",
    "            password=os.getenv('POSTGRES_PASSWORD'),\n",
    "            host=os.getenv('POSTGRES_HOST')\n",
    "        )\n",
    "\n",
    "        # Cursor set for SQL insertion\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected...\")\n",
    "\n",
    "        # DB tables to house both BPM 'Heart', Calories, and Geographic 'Trip' data are created\n",
    "        # Primary integer key is set for all tables\n",
    "        hearttable_sql='''\n",
    "                        CREATE SEQUENCE heart_timeID_seq\n",
    "                        START WITH 0\n",
    "                        INCREMENT BY 1\n",
    "                        MINVALUE 0\n",
    "                        NO MAXVALUE\n",
    "                        CACHE 1;\n",
    "        \n",
    "                        CREATE TABLE IF NOT EXISTS\n",
    "                            heart(\n",
    "                                time_id INTEGER PRIMARY KEY DEFAULT nextval('heart_timeID_seq'),\n",
    "                                dateTime TIMESTAMP,\n",
    "                                Bpm INTEGER,\n",
    "                                Confidence INTEGER\n",
    "                            );\n",
    "                    '''\n",
    "        cursor.execute(hearttable_sql)\n",
    "        conn.commit()\n",
    "        print(\"Created HEART Table...\")\n",
    "\n",
    "        calorietable_sql='''\n",
    "                        CREATE SEQUENCE calorie_timeID_seq\n",
    "                        START WITH 0\n",
    "                        INCREMENT BY 1\n",
    "                        MINVALUE 0\n",
    "                        NO MAXVALUE\n",
    "                        CACHE 1;\n",
    "        \n",
    "                        CREATE TABLE IF NOT EXISTS\n",
    "                            calorie(\n",
    "                                time_id INTEGER PRIMARY KEY DEFAULT nextval('calorie_timeID_seq'),\n",
    "                                dateTime TIMESTAMP,\n",
    "                                Calories FLOAT\n",
    "                            );\n",
    "                    '''\n",
    "        cursor.execute(calorietable_sql)\n",
    "        conn.commit()\n",
    "        print(\"Created CALORIE Table...\")\n",
    "\n",
    "        triptable_sql='''\n",
    "                        CREATE SEQUENCE trip_timeID_seq\n",
    "                        START WITH 0\n",
    "                        INCREMENT BY 1\n",
    "                        MINVALUE 0\n",
    "                        NO MAXVALUE\n",
    "                        CACHE 1;\n",
    "        \n",
    "                        CREATE TABLE IF NOT EXISTS\n",
    "                            trip(\n",
    "                                time_id INTEGER PRIMARY KEY DEFAULT nextval('trip_timeID_seq'),\n",
    "                                name TEXT,\n",
    "                                address TEXT,\n",
    "                                description TEXT,\n",
    "                                TimeSpan_begin TIMESTAMP,\n",
    "                                TimeSpan_end TIMESTAMP,\n",
    "                                calories DOUBLE PRECISION,\n",
    "                                avg_hr NUMERIC,\n",
    "                                avg_conf NUMERIC,\n",
    "                                longitude FLOAT,\n",
    "                                latitude FLOAT,\n",
    "                                LineString_coordinates TEXT\n",
    "                            );\n",
    "                    '''\n",
    "        cursor.execute(triptable_sql)\n",
    "        conn.commit()\n",
    "        print(\"Created TRIP Table...\")\n",
    "\n",
    "    except psycopg2.Error as error:\n",
    "            print(\"Modular Error...\")\n",
    "            print(error)\n",
    "\n",
    "    # Database resources are closed at the end of insertion cycles\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "            if conn:\n",
    "                conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility establishing database connection,cursor function returning all access objects\n",
    "def connect_PostgreSQL():\n",
    "    try:\n",
    "        dbconn = psycopg2.connect(\n",
    "            database=os.getenv('POSTGRES_DB'),\n",
    "            user=os.getenv('POSTGRES_USER'),\n",
    "            password=os.getenv('POSTGRES_PASSWORD'),\n",
    "            host=os.getenv('POSTGRES_HOST')\n",
    "        )\n",
    "        cursor = dbconn.cursor()\n",
    "        #print(\"Connected to 'heartcaltrip' database\")\n",
    "    except psycopg2.Error as error:\n",
    "        print(\"Error opening database\")\n",
    "        dbconn = handle_DB_error(dbconn, cursor)\n",
    "    return dbconn, cursor\n",
    "\n",
    "# Utility to close database resources to call after future insertions\n",
    "def close_DB_resources(dbconn, cursor):\n",
    "    try:\n",
    "        if dbconn:\n",
    "            dbconn.close()\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        #print(\"closed resources\")\n",
    "    except psycopg2.Error as err:\n",
    "        (\"Error closing resources\")\n",
    "\n",
    "# Utility to handle database errors\n",
    "def handle_DB_error(dbconn, cursor):\n",
    "    if dbconn:\n",
    "        try:\n",
    "            dbconn.rollback()\n",
    "            print(\"Rolled back transation\")\n",
    "        except psycopg2.Error as error:\n",
    "            print(\"Error rolling back transaction\")\n",
    "        finally:\n",
    "            close_DB_resources(dbconn, cursor)\n",
    "            dbconn = None \n",
    "            return dbconn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pg_insert_heart_csv(heart_csv):\n",
    "    \n",
    "    heart_insert = '''INSERT INTO heart (time_id, dateTime, Bpm, Confidence)\n",
    "                      VALUES (DEFAULT,%s,%s,%s);\n",
    "                    '''\n",
    "\n",
    "    # BRM 'Heart' table is populated first from CSV...\n",
    "    dbconn, cursor = connect_PostgreSQL()\n",
    "\n",
    "    with open(heart_csv) as file:\n",
    "        time_stamps = csv.DictReader(file)\n",
    "        value_id = 0\n",
    "        for time in time_stamps:\n",
    "            value_id += 1\n",
    "            record = (time['dateTime'], time['BPM'], time['Confidence'])\n",
    "            cursor.execute(heart_insert, record)\n",
    "            dbconn.commit()\n",
    "        #print('All file records committed to database: BPM/HeartRate table...')\n",
    "\n",
    "    close_DB_resources(dbconn, cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pg_insert_calorie_csv(calorie_csv):\n",
    "    \n",
    "    calorie_insert = '''INSERT INTO calorie (time_id, dateTime, Calories)\n",
    "                        VALUES (DEFAULT,%s,%s);\n",
    "                    '''\n",
    "    # BRM 'Heart' table is populated first from CSV...\n",
    "    dbconn, cursor = connect_PostgreSQL()\n",
    "\n",
    "    with open(calorie_csv) as file:\n",
    "        time_stamps = csv.DictReader(file)\n",
    "        value_id = 0\n",
    "        for time in time_stamps:\n",
    "            value_id += 1\n",
    "            record = (time['dateTime'], time['Calories'])\n",
    "            cursor.execute(calorie_insert, record)\n",
    "            dbconn.commit()\n",
    "        #print('All file records committed to database: Caloric table...')\n",
    "\n",
    "    close_DB_resources(dbconn, cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pg_insert_location_csv(Google_location_csv):\n",
    "    \n",
    "    trip_insert = '''INSERT INTO trip (time_id, name, address, description, TimeSpan_begin, TimeSpan_end, longitude, latitude, LineString_coordinates)\n",
    "                     VALUES (DEFAULT,%s,%s,%s,%s,%s,%s,%s,%s);\n",
    "                    '''\n",
    "    \n",
    "    dbconn, cursor = connect_PostgreSQL()\n",
    "\n",
    "    with open(Google_location_csv) as file:\n",
    "        time_stamps = csv.DictReader(file)\n",
    "        value_id = 0\n",
    "        for time in time_stamps:\n",
    "            try:\n",
    "                float(time['longitude'])\n",
    "            except:\n",
    "                #print('invalid entry')\n",
    "                time['longitude'] = None\n",
    "                time['latitude'] = None\n",
    "\n",
    "            record = (time['name'], time['address'], time['description'], time['TimeSpan/begin'], time['TimeSpan/end'], time['longitude'], time['latitude'], time['LineString/coordinates'])\n",
    "            cursor.execute(trip_insert, record)\n",
    "            dbconn.commit()\n",
    "        #print('All file records committed to database: Geographic/Location table...')\n",
    "\n",
    "    close_DB_resources(dbconn, cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Postgres_combine_view():\n",
    "    # Readings within 'dateTime' are truncated down to their minute value\n",
    "    # Average values for BPM and Confidence readings are taken per minute period\n",
    "    # Summation of Calorie values are taken over for every minute period\n",
    "    # All values are singularly grouped for every minute, then ordered chronologically...\n",
    "\n",
    "    # These values are populated as single postgres View resource...\n",
    "    create_calorie_heartrate_view = '''CREATE VIEW calorie_heartrate AS\n",
    "                     SELECT date_trunc('minute', h.dateTime) AS dateTime_min,\n",
    "                     avg(h.Bpm) AS BPM_min_avg, avg(h.Confidence) AS Conf_min_avg,\n",
    "                     avg(c.calories) AS calories\n",
    "                     FROM heart AS h\n",
    "                     JOIN calorie as c\n",
    "                     ON c.dateTime = date_trunc('minute', h.dateTime)\n",
    "                     GROUP BY date_trunc('minute', h.dateTime)\n",
    "                     ORDER BY date_trunc('minute', h.dateTime);\n",
    "                  ''' \n",
    "\n",
    "    # All newly-generated minute values for Calories, BPM, Confidence are collected from View...\n",
    "    pull_calorie_heartrate = '''SELECT dateTime_min, calories, BPM_min_avg, Conf_min_avg FROM calorie_heartrate;''' \n",
    "\n",
    "    # First insertion is executed, compended postgres View is created of most-relevant data\n",
    "    # View is then fetched, that data is then stored in a single tuple...\n",
    "\n",
    "    dbconn, cursor = connect_PostgreSQL()\n",
    "\n",
    "    cursor.execute(create_calorie_heartrate_view)\n",
    "    dbconn.commit()\n",
    "    print('Grouping data readings per minute...')\n",
    "    cursor.execute(pull_calorie_heartrate)\n",
    "    tupples = cursor.fetchall()\n",
    "    print('Extracting data approximations from Postgres...')\n",
    "\n",
    "    close_DB_resources(dbconn, cursor)\n",
    "    \n",
    "    # That tuple is used to create new pd Dataframe of Calories + Average HeartRate per minute...\n",
    "    calorie_heartrate_df = pd.DataFrame(tupples, columns=['dateTime','calories','bpm_min','conf_min'])\n",
    "    return calorie_heartrate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Postgres_populate_Trip_table():\n",
    "    \n",
    "    dbconn, cursor = connect_PostgreSQL()\n",
    "\n",
    "    trip_pull = '''SELECT * FROM trip\n",
    "                   ORDER BY time_id;'''\n",
    "\n",
    "    heart_cal_pull = '''SELECT sum(c_hr.calories) AS calories,\n",
    "                               avg(c_hr.bpm_min_avg) AS avg_bpm,\n",
    "                               avg(c_hr.conf_min_avg) AS confidence\n",
    "                        FROM calorie_heartrate AS c_hr\n",
    "                        WHERE c_hr.dateTime_min\n",
    "                        BETWEEN (%s) AND (%s);\n",
    "                     '''\n",
    "\n",
    "    trip_fill_columns =  '''UPDATE public.trip\n",
    "                            SET calories = CAST(%s AS DOUBLE PRECISION),\n",
    "                                avg_hr = (%s),\n",
    "                                avg_conf = (%s)\n",
    "                            WHERE\n",
    "                                time_id = (%s);\n",
    "                         '''\n",
    "\n",
    "    insertion_record = '''SELECT calories, avg_hr, avg_conf FROM trip\n",
    "                          WHERE time_id = (%s);\n",
    "                          '''\n",
    "\n",
    "    final_trip_pull = '''SELECT time_id, name, address, timespan_begin, timespan_end,\n",
    "                         calories, avg_hr, avg_conf, longitude, latitude, linestring_coordinates, description\n",
    "                         FROM trip ORDER BY time_id;\n",
    "                         '''\n",
    "\n",
    "    # Location 'Trip' table is scanned, iterated by its consecutive time periods at a location (or in transit)\n",
    "\n",
    "    # For each arbitrary time period expressed:\n",
    "    #    - Our Postgres View of minutely-segregated calories burned, and average BPM readings, are collected.\n",
    "    #        - Each per-minute reading is then again either summated or averaged to a single value.\n",
    "    #        - These values are used for insertion into their respective column on 'Trip' table, via 'heart_cal_period'.\n",
    "    #        - 'heart_cal_period' matches values within their appropriate time period.\n",
    "\n",
    "    cursor.execute(trip_pull)\n",
    "    trip_record = cursor.fetchall()\n",
    "\n",
    "    for row in trip_record:\n",
    "        cursor.execute(heart_cal_pull, (row[4], row[5]))\n",
    "        heart_cal_period = cursor.fetchone()\n",
    "        if heart_cal_period:\n",
    "            cursor.execute(trip_fill_columns, (heart_cal_period[0], heart_cal_period[1], heart_cal_period[2], row[0]))\n",
    "            cursor.execute(insertion_record, (row[0],))\n",
    "            #print(cursor.fetchone())\n",
    "\n",
    "    print(\"\\nColumns populated: 'calories', 'average_hr', 'avg_conf'\")\n",
    "    print(\"Total rows are: \", len(trip_record))\n",
    "\n",
    "    dbconn.commit()\n",
    "\n",
    "    cursor.execute(final_trip_pull)\n",
    "    trip_tupple = cursor.fetchall()\n",
    "\n",
    "    close_DB_resources(dbconn, cursor)\n",
    "    \n",
    "    total_trip_df = pd.DataFrame(trip_tupple, columns=['datetime_min','bpm_min_avg','conf_min_avg','calories','Time end','Calories','BPM avg','BPM conf','Longitude','Latitude','LineString_coord','Description'])\n",
    "    total_trip_df.to_csv('finished_table.csv')\n",
    "    # pandas dataframe for calories and heartrate also generated, if needed...\n",
    "    return total_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    Postgres_create_DB()\n",
    "    heart_jsons, calorie_jsons, location_kmls = generate_Fitbit_json_file_lists()\n",
    "    \n",
    "    file_id = 0\n",
    "    \n",
    "    for file in heart_jsons:\n",
    "        heart_result_csv = json_heart_convert_csv(file)\n",
    "        Pg_insert_heart_csv('working_heart.csv')\n",
    "        file_id += 1\n",
    "        print('Heart file', file_id, 'committed...')\n",
    "    print('Heart rate data committed to database.\\n')\n",
    "    file_id = 0\n",
    "    \n",
    "    for file in calorie_jsons:\n",
    "        calorie_result_csv = json_calorie_convert_csv(file)\n",
    "        Pg_insert_calorie_csv('working_calorie.csv')\n",
    "        file_id += 1\n",
    "        print('Caloric file', file_id, 'committed...')\n",
    "    print('Caloric data committed to database.\\n')\n",
    "    file_id = 0\n",
    "    \n",
    "    for file in location_kmls:\n",
    "        formatted_location_csv = csv_format_google_kml(file)\n",
    "        Pg_insert_location_csv('working_location.csv')\n",
    "        file_id += 1\n",
    "        print('Location file', file_id, 'committed...')\n",
    "    print('Location data committed to database.\\n')\n",
    "\n",
    "    Postgres_combine_view()\n",
    "    Postgres_populate_Trip_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full CSV is loaded, duplicates are dropped by matching 'Time begin' values\n",
    "# Rows are sorted by 'Time begin' value, extra hanging indices are dropped\n",
    "\n",
    "total = (pd.read_csv('finished_table.csv')).drop_duplicates(['Time begin'])\n",
    "total = (total.sort_values(['Time begin'])).reset_index()\n",
    "total_trip_df = (total.drop(['index','Unnamed: 0','time_id'],1))\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Empty Fitbit cells are identified, set to average of all values of that type\n",
    "\n",
    "null_rows_cal = total_trip_df['Calories'].isnull()\n",
    "null_rows_avg = total_trip_df['BPM avg'].isnull()\n",
    "null_rows_conf = total_trip_df['BPM conf'].isnull()\n",
    "\n",
    "for i, value in enumerate(total_trip_df['Calories']):\n",
    "    if null_rows_cal[i]:\n",
    "        total_trip_df['Calories'][i] = (total_trip_df['Calories']).mean()\n",
    "for i, value in enumerate(total_trip_df['BPM avg']):\n",
    "    if null_rows_avg[i]:\n",
    "        total_trip_df['BPM avg'][i] = (total_trip_df['BPM avg']).mean()\n",
    "for i, value in enumerate(total_trip_df['BPM conf']):\n",
    "    if null_rows_conf[i]:\n",
    "        total_trip_df['BPM conf'][i] = (total_trip_df['BPM conf']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import MarkerCluster,HeatMap\n",
    "from folium import Marker\n",
    "from branca.element import Figure\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates separate dataframes for stationary and transit periods\n",
    "# Stationary/transit periods are separated by null value in \"Lat/Long\" columns\n",
    "# Stationary Lat+Long are simply listed with BPM value (scaled for map)\n",
    "# Transit period \"LineString_coord\" values are isolated, split by their dividers\n",
    "# Transit Lat+Long are made into lists all sharing BPM for that time period\n",
    "\n",
    "stationary_rows = []\n",
    "stationary_readings = []\n",
    "transit_rows = []\n",
    "transit_readings = []\n",
    "\n",
    "for idx, row in total_trip_df.iterrows():\n",
    "    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n",
    "        st_row = [row['Name'], row['Address'], row['Time begin'], row['Time end'], row['Calories'], row['BPM avg'], row['BPM conf'], row['Longitude'], row['Latitude'], row['LineString_coord'], row['Description']]\n",
    "        stationary_rows.append(st_row)\n",
    "        stationary_trip_df = pd.DataFrame(stationary_rows, columns=['Name','Address','Time begin','Time end','Calories','BPM avg','BPM conf','Longitude','Latitude','LineString_coord','Description'])\n",
    "        stationary_trip_df = stationary_trip_df.sort_values('Time begin')\n",
    "        st_readings = [row['Latitude'], row['Longitude'], ((row['BPM avg'])/10000)]\n",
    "        stationary_readings.append(st_readings)\n",
    "    else:\n",
    "        tr_row = [row['Name'], row['Address'], row['Time begin'], row['Time end'], row['Calories'], row['BPM avg'], row['BPM conf'], row['Longitude'], row['Latitude'], row['LineString_coord'], row['Description']]\n",
    "        transit_rows.append(tr_row)\n",
    "        transit_trip_df = pd.DataFrame(transit_rows, columns=['Name','Address','Time begin','Time end','Calories','BPM avg','BPM conf','Longitude','Latitude','LineString_coord','Description'])\n",
    "        transit_trip_df = transit_trip_df.sort_values('Time begin')\n",
    "        tran_reading = row['LineString_coord']\n",
    "        tran_list = tran_reading.split()\n",
    "        for t in tran_list:\n",
    "            new = (t[:-2]).split(',')\n",
    "            for t in new:\n",
    "                lat_float = float(new[1])\n",
    "                long_float = float(new[0])\n",
    "            tran_reading = (lat_float, long_float, ((row['BPM avg'])/10000))\n",
    "            transit_readings.append(tran_reading)\n",
    "        for t in transit_readings:\n",
    "            t = list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuples are redefined as lists and fed (lat, long, bpm) into HeatMap function\n",
    "\n",
    "print('Stationary readings:', len(stationary_readings))\n",
    "print('Transitory readings:', len(transit_readings))\n",
    "\n",
    "for t in stationary_readings:\n",
    "    t = list(t)\n",
    "for t in transit_readings:\n",
    "    t = list(t)\n",
    "    \n",
    "m=folium.Map(width=825,height=825,location=[39.7392, -104.9903],zoom_start=4)\n",
    "plugins.HeatMap(stationary_readings).add_to(m)\n",
    "plugins.HeatMap(transit_readings).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import *\n",
    "gis = GIS(\"https://www.arcgis.com\", \"steven.wortmann\", \"Paigek#624\")\n",
    "m = gis.map(\"Pittsburgh, PA\",mode='2D')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [stationary_trip_df['Latitude'], stationary_trip_df['Longitude']]\n",
    "points = gis.content.import_data(df)\n",
    "m.add_layer(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = gis.content.search(query=\"landsat usa\", item_type=\"Feature Layer\", max_items = 100)\n",
    "for item in items:\n",
    "    display(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import *\n",
    "\n",
    "gis = GIS()\n",
    "m = gis.map('United States')\n",
    "\n",
    "#locations = [stationary_trip_df['Latitude'], stationary_trip_df['Longitude']]\n",
    "#df = pd.DataFrame.from_records(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
